{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb23396e-9baa-4d7c-8631-f13fa48a1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from pyspark.sql import SparkSession, DataFrameWriter\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee75748-7a9f-464b-bd2e-9896cff363fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5a34cbc5025c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>extract-transform</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3594334cd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание спарк сессии\n",
    "spark = SparkSession.builder.master(\"local\").enableHiveSupport().appName(\"extract-transform\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e7d4a7-4286-4214-b6bc-12f0bfd17650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем данные из паркета\n",
    "df = spark.read.format(\"parquet\").load('data_in/competition_data_final_pqt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "725ce023-a74d-4068-ba77-ae3d84710542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- region_name: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- cpe_manufacturer_name: string (nullable = true)\n",
      " |-- cpe_model_name: string (nullable = true)\n",
      " |-- url_host: string (nullable = true)\n",
      " |-- cpe_type_cd: string (nullable = true)\n",
      " |-- cpe_model_os_type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- part_of_day: string (nullable = true)\n",
      " |-- request_cnt: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ca2c0d-45f0-4b44-810e-8bcb6fc1442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем локальное представление датафрейма, как sql таблицы mts\n",
    "df.createOrReplaceTempView(\"mts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f678ac8-28c9-4e64-822b-7ba8974ab96d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = spark.sql(\"select user_id, max(price) as price from mts group by user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe13575a-0ecb-4624-9169-b9ba0e7390cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415317"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21496c5b-47a3-419e-a9c7-03840f4a5467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub_1 = spark.sql(\"select user_id, max(request_cnt) as max_request_cnt, round(avg(request_cnt), 3) as avg_request_cnt, count(request_cnt)as count_request_cnt \"\n",
    "    \" from mts group by user_id\")\n",
    "data_learn = data.select(\"user_id\", \"price\").join(df_sub_1, \"user_id\", 'left')\n",
    "df_sub_2 = spark.sql(\"select user_id, max(request_cnt) as max_night_request_cnt, \"\n",
    "    \" round(avg(request_cnt), 3) as avg_night_request_cnt, count(request_cnt)as count_night_request_cnt \"\n",
    "    \" from mts where part_of_day = 'night' group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_2, \"user_id\", 'left')\n",
    "df_sub_3 = spark.sql(\"select user_id, max(request_cnt) as max_day_request_cnt, \"\n",
    "    \" round(avg(request_cnt), 3) as avg_day_request_cnt, count(request_cnt)as count_day_request_cnt \"\n",
    "    \" from mts where part_of_day = 'day' group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_3, \"user_id\", 'left')\n",
    "df_sub_4 = spark.sql(\"select user_id, max(request_cnt) as max_morning_request_cnt, \"\n",
    "    \" round(avg(request_cnt), 3) as avg_morning_request_cnt, count(request_cnt)as count_morning_request_cnt \"\n",
    "    \" from mts where part_of_day = 'morning' group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_4, \"user_id\", 'left')\n",
    "df_sub_5 = spark.sql(\"select user_id, max(request_cnt) as max_evening_request_cnt, \"\n",
    "    \" round(avg(request_cnt), 3) as avg_evening_request_cnt, count(request_cnt)as count_evening_request_cnt \"\n",
    "    \" from mts where part_of_day = 'evening' group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_5, \"user_id\", 'left')\n",
    "df_sub_6 = spark.sql(\"select user_id, max(sum_date_request_cnt) as max_sum_date_request_cnt, min(sum_date_request_cnt) as min_sum_date_request_cnt, \"\n",
    "    \" round(avg(sum_date_request_cnt), 3) as avg_sum_date_request_cnt\"\n",
    "    \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts group by user_id, date) as t1\"\n",
    "    \" group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_6, \"user_id\", 'left')\n",
    "df_sub_7 = spark.sql(\"select user_id, max(sum_date_request_cnt) as max_sum_date_day_request_cnt, min(sum_date_request_cnt) as min_sum_date_day_request_cnt, \"\n",
    "    \" round(avg(sum_date_request_cnt), 3) as avg_sum_date_day_request_cnt\"\n",
    "    \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts where part_of_day = 'day' group by user_id, date) as t2\"\n",
    "    \" group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_7, \"user_id\", 'left')\n",
    "df_sub_8 = spark.sql(\"select user_id, max(sum_date_request_cnt) as max_sum_date_night_request_cnt, min(sum_date_request_cnt) as min_sum_date_night_request_cnt, \"\n",
    "    \"round(avg(sum_date_request_cnt), 3) as avg_sum_date_night_request_cnt\"\n",
    "    \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts where part_of_day = 'night' group by user_id, date) as t3\"\n",
    "    \" group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_8, \"user_id\", 'left')\n",
    "df_sub_9 = spark.sql(\"select user_id, max(sum_date_request_cnt) as max_sum_date_morning_request_cnt, min(sum_date_request_cnt) as min_sum_date_morning_request_cnt, \"\n",
    "    \" round(avg(sum_date_request_cnt), 3) as avg_sum_date_morning_request_cnt\"\n",
    "    \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts where part_of_day = 'morning' group by user_id, date) as t4\"\n",
    "    \" group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_9, \"user_id\", 'left')\n",
    "df_sub_10 = spark.sql(\"select user_id, max(sum_date_request_cnt) as max_sum_date_evening_request_cnt, min(sum_date_request_cnt) as min_sum_date_evening_request_cnt, \"\n",
    "    \" round(avg(sum_date_request_cnt), 3) as avg_sum_date_evening_request_cnt\"\n",
    "    \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts where part_of_day = 'evening' group by user_id, date) as t5\"\n",
    "    \" group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_10, \"user_id\", 'left')\n",
    "df_sub_11 = spark.sql(\"select user_id, count(date) as count_date\"\n",
    "    \" from (select user_id, date from mts group by user_id, date) as t6 group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_11, \"user_id\", 'left')\n",
    "df_sub_12 = spark.sql(\"select user_id, count(date) as count_day_date\"\n",
    "    \" from (select user_id, date from mts where part_of_day = 'day' group by user_id, date) as t7 group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_12, \"user_id\", 'left')\n",
    "df_sub_13 = spark.sql(\"select user_id, count(date) as count_night_date\"\n",
    "    \" from (select user_id, date from mts where part_of_day = 'night' group by user_id, date) as t8 group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_13, \"user_id\", 'left')\n",
    "df_sub_14 = spark.sql(\"select user_id, count(date) as count_morning_date\"\n",
    "    \" from (select user_id, date from mts where part_of_day = 'morning' group by user_id, date) as t9 group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_14, \"user_id\", 'left')\n",
    "df_sub_15 = spark.sql(\"select user_id, count(date) as count_evening_date\"\n",
    "    \" from (select user_id, date from mts where part_of_day = 'evening' group by user_id, date) as t10 group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_15, \"user_id\", 'left')\n",
    "df_sub_16 = spark.sql(\"select user_id, avg(count_part_of_day_date) as avg_count_part_of_day_date,\"\n",
    "    \" max(count_part_of_day_date) as max_count_part_of_day_date, min(count_part_of_day_date) as min_count_part_of_day_date\"\n",
    "    \" from (select user_id, date, count(part_of_day) as count_part_of_day_date\"\n",
    "    \" from (select user_id, date, part_of_day from mts group by user_id, date, part_of_day) as t11\"\n",
    "    \" group by user_id, date) as t12 group by user_id\")\n",
    "data_learn = data_learn.join(df_sub_16, \"user_id\", 'left')\n",
    "df_sub_17 = spark.sql(\"select user_id, avg(lag_date) as avg_lag_date, max(lag_date) as max_lag_date, min(lag_date) as min_lag_date\"\n",
    "    \" from (select user_id, int(date - lag(date) over (partition by user_id order by date)) as lag_date\"\n",
    "    \" from (select user_id, date from mts group by user_id, date order by user_id, date) as t13) as t14\"\n",
    "    \" group by user_id order by user_id\")\n",
    "data_learn = data_learn.join(df_sub_17, \"user_id\", 'left')\n",
    "df_sub_18 = spark.sql(\"select user_id, count(region_name) as count_region_name\"\n",
    "    \" from (select user_id, region_name from mts group by user_id, region_name) as t15\"\n",
    "    \" group by user_id order by user_id\")\n",
    "data_learn = data_learn.join(df_sub_18, \"user_id\", 'left')\n",
    "df_sub_19 = spark.sql(\"select user_id, count(city_name) as count_city_name\"\n",
    "    \" from (select user_id, city_name from mts group by user_id, city_name) as t16\"\n",
    "    \" group by user_id order by user_id\")\n",
    "data_learn = data_learn.join(df_sub_19, \"user_id\", 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1cda35d-a78e-4190-87bb-945787a0eb66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 138 ms, sys: 75.4 ms, total: 213 ms\n",
      "Wall time: 17min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_learn.write.parquet(path=\"data_out/data_for_learn_parquet_last\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b90cace8-2299-4e99-9352-8f9e21ac9ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_test = spark.read.format(\"parquet\").load('data_out/data_for_learn_parquet_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe25ce0-1d46-4a7f-a6e6-c1cb6c0f7df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415317"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a650c81-af71-4cc7-8ad2-31cbeea37a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a618f-c59a-46c9-a120-c17dad81472e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7daa176-8d5c-4c82-b61b-313e73d45f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------\n",
      " user_id           | 99002  \n",
      " age               | 41.0   \n",
      " is_male           | 0      \n",
      " max_request_cnt   | 8      \n",
      " avg_request_cnt   | 1.34   \n",
      " count_request_cnt | 639    \n",
      "-RECORD 1-------------------\n",
      " user_id           | 155506 \n",
      " age               | 33.0   \n",
      " is_male           | 0      \n",
      " max_request_cnt   | 5      \n",
      " avg_request_cnt   | 1.727  \n",
      " count_request_cnt | 22     \n",
      "-RECORD 2-------------------\n",
      " user_id           | 188276 \n",
      " age               | 35.0   \n",
      " is_male           | 1      \n",
      " max_request_cnt   | 4      \n",
      " avg_request_cnt   | 1.414  \n",
      " count_request_cnt | 111    \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data_learn = spark.sql(\"with sub_1 as\"\n",
    "#     \" (select user_id, max(request_cnt) as max_request_cnt, round(avg(request_cnt), 3) as avg_request_cnt, count(request_cnt)as count_request_cnt \"\n",
    "#     \" from mts group by user_id)\"\n",
    "#     \", sub_2 as (select user_id, max(request_cnt) as max_night_request_cnt, \"\n",
    "#     \" round(avg(request_cnt), 3) as avg_night_request_cnt, count(request_cnt)as count_night_request_cnt \"\n",
    "#     \" from mts where part_of_day = 'night' group by user_id)\"\n",
    "    # \", sub_3 as (select user_id, max(request_cnt) as max_day_request_cnt, \"\n",
    "    # \" round(avg(request_cnt), 3) as avg_day_request_cnt, count(request_cnt)as count_day_request_cnt \"\n",
    "    # \" from mts where part_of_day = 'day' group by user_id)\"\n",
    "    # \", sub_4 as (select user_id, max(request_cnt) as max_morning_request_cnt, \"\n",
    "    # \" round(avg(request_cnt), 3) as avg_morning_request_cnt, count(request_cnt)as count_morning_request_cnt \"\n",
    "    # \" from mts where part_of_day = 'morning' group by user_id)\"\n",
    "    # \", sub_5 as (select user_id, max(request_cnt) as max_evening_request_cnt, \"\n",
    "    # \" round(avg(request_cnt), 3) as avg_evening_request_cnt, count(request_cnt)as count_evening_request_cnt \"\n",
    "    # \" from mts where part_of_day = 'evening' group by user_id)\"\n",
    "    # \", sub_6 as (select user_id, max(sum_date_request_cnt) as max_sum_date_request_cnt, min(sum_date_request_cnt) as min_sum_date_request_cnt, \"\n",
    "    # \" round(avg(sum_date_request_cnt), 3) as avg_sum_date_request_cnt\"\n",
    "    # \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts group by user_id, date) as t1\"\n",
    "    # \" group by user_id)\"\n",
    "    # \", sub_7 as (select user_id, max(sum_date_request_cnt) as max_sum_date_day_request_cnt, min(sum_date_request_cnt) as min_sum_date_day_request_cnt, \"\n",
    "    # \" round(avg(sum_date_request_cnt), 3) as avg_sum_date_day_request_cnt\"\n",
    "    # \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts where part_of_day = 'day' group by user_id, date) as t2\"\n",
    "    # \" group by user_id)\"\n",
    "    # \", sub_8 as (select user_id, max(sum_date_request_cnt) as max_sum_date_night_request_cnt, min(sum_date_request_cnt) as min_sum_date_night_request_cnt, \"\n",
    "    # \"round(avg(sum_date_request_cnt), 3) as avg_sum_date_night_request_cnt\"\n",
    "    # \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts where part_of_day = 'night' group by user_id, date) as t3\"\n",
    "    # \" group by user_id)\"\n",
    "    # \", sub_9 as (select user_id, max(sum_date_request_cnt) as max_sum_date_morning_request_cnt, min(sum_date_request_cnt) as min_sum_date_morning_request_cnt, \"\n",
    "    # \" round(avg(sum_date_request_cnt), 3) as avg_sum_date_morning_request_cnt\"\n",
    "    # \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts where part_of_day = 'morning' group by user_id, date) as t4\"\n",
    "    # \" group by user_id)\"\n",
    "    # \", sub_10 as (select user_id, max(sum_date_request_cnt) as max_sum_date_evening_request_cnt, min(sum_date_request_cnt) as min_sum_date_evening_request_cnt, \"\n",
    "    # \" round(avg(sum_date_request_cnt), 3) as avg_sum_date_evening_request_cnt\"\n",
    "    # \" from (select user_id, date, sum(request_cnt) as sum_date_request_cnt from mts where part_of_day = 'evening' group by user_id, date) as t5\"\n",
    "    # \" group by user_id)\"\n",
    "    # \", sub_11 as (select user_id, count(date) as count_date\"\n",
    "    # \" from (select user_id, date from mts group by user_id, date) as t6 group by user_id)\"\n",
    "    # \", sub_12 as (select user_id, count(date) as count_day_date\"\n",
    "    # \" from (select user_id, date from mts where part_of_day = 'day' group by user_id, date) as t7 group by user_id)\"\n",
    "    # \", sub_13 as (select user_id, count(date) as count_night_date\"\n",
    "    # \" from (select user_id, date from mts where part_of_day = 'night' group by user_id, date) as t8 group by user_id)\"\n",
    "    # \", sub_14 as (select user_id, count(date) as count_morning_date\"\n",
    "    # \" from (select user_id, date from mts where part_of_day = 'morning' group by user_id, date) as t9 group by user_id)\"\n",
    "    # \", sub_15 as (select user_id, count(date) as count_evening_date\"\n",
    "    # \" from (select user_id, date from mts where part_of_day = 'evening' group by user_id, date) as t10 group by user_id)\"\n",
    "    # \", sub_16 as (select user_id, avg(count_part_of_day_date) as avg_count_part_of_day_date,\"\n",
    "    # \" max(count_part_of_day_date) as max_count_part_of_day_date, min(count_part_of_day_date) as min_count_part_of_day_date\"\n",
    "    # \" from (select user_id, date, count(part_of_day) as count_part_of_day_date\"\n",
    "    # \" from (select user_id, date, part_of_day from mts group by user_id, date, part_of_day) as t11\"\n",
    "    # \" group by user_id, date) as t12 group by user_id)\"\n",
    "    # \", sub_17 as (select user_id, avg(lag_date) as avg_lag_date, max(lag_date) as max_lag_date, min(lag_date) as min_lag_date\"\n",
    "    # \" from (select user_id, int(date - lag(date) over (partition by user_id order by date)) as lag_date\"\n",
    "    # \" from (select user_id, date from mts group by user_id, date order by user_id, date) as t13) as t14\"\n",
    "    # \" group by user_id order by user_id)\"\n",
    "    # \", sub_18 as (select user_id, count(region_name) as count_region_name\"\n",
    "    # \" from (select user_id, region_name from mts group by user_id, region_name) as t15\"\n",
    "    # \" group by user_id order by user_id)\"\n",
    "    # \", sub_19 as (select user_id, count(city_name) as count_city_name\"\n",
    "    # \" from (select user_id, city_name from mts group by user_id, city_name) as t16\"\n",
    "    # \" group by user_id order by user_id)\"                     \n",
    "    # ).show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc80bbb7-29d0-4fe6-bb13-5a20ef6994b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " user_id           | 0     \n",
      " age               | 35.0  \n",
      " is_male           | 0     \n",
      " max_request_cnt   | 5     \n",
      " avg_request_cnt   | 1.473 \n",
      " count_request_cnt | 131   \n",
      "-RECORD 1------------------\n",
      " user_id           | 1     \n",
      " age               | 41.0  \n",
      " is_male           | 0     \n",
      " max_request_cnt   | 6     \n",
      " avg_request_cnt   | 1.496 \n",
      " count_request_cnt | 700   \n",
      "-RECORD 2------------------\n",
      " user_id           | 2     \n",
      " age               | 25.0  \n",
      " is_male           | 0     \n",
      " max_request_cnt   | 4     \n",
      " avg_request_cnt   | 1.154 \n",
      " count_request_cnt | 356   \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_test.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0efaf-6648-4a62-bab4-4116ac73f436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
